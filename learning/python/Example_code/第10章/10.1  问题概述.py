# -*- coding: utf-8 -*-
# @Time : 2020/7/11 20:22
# @Author : MYH
# @File : 10.1  问题概述.py
# @Software: PyCharm

# 计算机会按照人类指令做事 这一讲实现网络爬虫问题引入
# python语言简洁性和脚本特点非常适合链接和网页处理 因此在python生态中,关于URL和网页处理相关的第三方库很多
# 本章讲解requests库和beautifulsoup4库,它们都是第三方库

# 网络爬虫应用一般分为两个步骤: 1. 通过网络链接获取网页内容 2. 对获得的网页内容进行处理.这两个功能需要用到安装的第三方库

# robots协议/爬虫协议 网站管理者表达是否希望爬虫自动获取网络信息医院的方法
# 如果管理者愿意,可以在网站根目录下放置一个robots.txt文件 并在此列出哪些链接不允许爬虫爬取
# 爬虫一般会先捕获这个文件并根据文本内容爬取网站内容 如果没有这个文件则表示网站可以被爬虫获得
# 这是一个道德规范准则,绝大多数搜索引擎爬虫都会遵循这个协议,建议个人也遵顼这个协议